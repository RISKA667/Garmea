#!/usr/bin/env python3
"""
CodexGenea - Parseur G√©n√©alogique Avanc√©
=========================================

Module principal pour l'analyse et le traitement de documents g√©n√©alogiques
fran√ßais de l'Ancien R√©gime. Ce parseur sp√©cialis√© traite les registres
paroissiaux, actes de bapt√™me, mariage et s√©pulture avec correction OCR
int√©gr√©e et extraction intelligente des informations familiales.

Fonctionnalit√©s principales:
- Lecture et traitement de documents PDF et texte
- Correction automatique des erreurs OCR
- Extraction et normalisation des noms de personnes
- Reconnaissance des titres nobiliaires et religieux
- Gestion des abr√©viations historiques
- Export multi-formats (JSON, TXT, GEDCOM)
- Validation chronologique des donn√©es
- Cache intelligent pour optimiser les performances

Auteur: Garm√©a Parser Team
Version: 3.0.0
Licence: MIT
Date: 2024-2025

Exemples d'utilisation:
    python main.py document.pdf                    # Traitement PDF complet
    python main.py document.txt -o resultats/      # Fichier texte avec sortie personnalis√©e
    python main.py document.pdf --pdf-pages 50     # Limiter √† 50 pages
    python main.py demo                            # Mode d√©monstration
    python main.py --test                          # Tests int√©gr√©s
"""

import argparse
import json
import logging
import re
import sys
import time
import traceback
from contextlib import contextmanager
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional, Union, Any, Tuple, Callable
import warnings
from functools import lru_cache, wraps
from collections import defaultdict, Counter
import hashlib

# Suppression des avertissements non critiques
warnings.filterwarnings('ignore', category=UserWarning)
warnings.filterwarnings('ignore', category=DeprecationWarning)

# === IMPORTS CONDITIONNELS ===
try:
    import fitz  # PyMuPDF pour la lecture PDF
    HAS_PYMUPDF = True
except ImportError:
    HAS_PYMUPDF = False

try:
    from parsers.base.text_parser import TextParser
    from parsers.base.name_extractor import NameExtractor  
    from database.person_manager import PersonManager
    HAS_PARSERS = True
except ImportError as e:
    print(f"Modules parsers manquants: {e}")
    HAS_PARSERS = False

# === CONSTANTES GLOBALES ===
VERSION = "3.0.0"
AUTHOR = "Garm√©a Parser Team"
LICENSE = "MIT"

# Codes de sortie standardis√©s
EXIT_SUCCESS = 0
EXIT_ERROR = 1
EXIT_INTERRUPTED = 130
EXIT_CONFIG_ERROR = 2
EXIT_DEPENDENCY_ERROR = 3


class Config:
    """
    Configuration centralis√©e du parseur g√©n√©alogique.
    
    Cette classe contient toutes les constantes et param√®tres de configuration
    utilis√©s par le syst√®me. Elle permet une gestion centralis√©e et coh√©rente
    des param√®tres de traitement.
    
    Attributes:
        DEFAULT_OUTPUT_DIR: R√©pertoire de sortie par d√©faut
        DEFAULT_LOGS_DIR: R√©pertoire des logs par d√©faut
        DEFAULT_CONFIG_FILE: Fichier de configuration par d√©faut
        MAX_PDF_PAGES: Nombre maximum de pages PDF √† traiter
        MAX_TEXT_LENGTH: Longueur maximale du texte √† traiter
        CHUNK_SIZE: Taille des chunks pour le traitement par blocs
        CACHE_SIZE: Taille du cache pour optimiser les performances
        ENABLE_OCR_CORRECTIONS: Activation des corrections OCR
        ENABLE_VALIDATION: Activation de la validation des donn√©es
        SUPPORTED_TEXT_FORMATS: Formats de texte support√©s
        SUPPORTED_PDF_FORMATS: Formats PDF support√©s
    """
    
    # === R√âPERTOIRES ===
    DEFAULT_OUTPUT_DIR = Path("output")
    DEFAULT_LOGS_DIR = Path("logs")
    DEFAULT_CONFIG_FILE = Path("config/settings.json")
    
    # === LIMITES DE TRAITEMENT ===
    MAX_PDF_PAGES = 500
    MAX_TEXT_LENGTH = 1_000_000
    CHUNK_SIZE = 100_000
    CACHE_SIZE = 5000
    
    # === OPTIONS DE TRAITEMENT ===
    ENABLE_OCR_CORRECTIONS = True
    ENABLE_VALIDATION = True
    
    # === FORMATS SUPPORT√âS ===
    SUPPORTED_TEXT_FORMATS = {'.txt', '.md', '.rtf'}
    SUPPORTED_PDF_FORMATS = {'.pdf'}
    
    # === SEUILS DE QUALIT√â ===
    MIN_NAME_CONFIDENCE = 0.6
    MIN_DATE_CONFIDENCE = 0.7
    MIN_RELATIONSHIP_CONFIDENCE = 0.8
    
    # === PARAM√àTRES DE PERFORMANCE ===
    PROGRESS_UPDATE_INTERVAL = 0.5  # secondes
    MEMORY_CLEANUP_THRESHOLD = 1000  # documents
    
    @classmethod
    def get_all_supported_formats(cls) -> set:
        """
        Retourne l'ensemble de tous les formats support√©s.
        
        Returns:
            set: Ensemble des extensions de fichiers support√©es
        """
        return cls.SUPPORTED_TEXT_FORMATS | cls.SUPPORTED_PDF_FORMATS
    
    @classmethod
    def validate_config(cls, config_dict: Dict[str, Any]) -> Dict[str, Any]:
        """
        Valide et normalise une configuration.
        
        Args:
            config_dict: Dictionnaire de configuration √† valider
            
        Returns:
            Dict[str, Any]: Configuration valid√©e et normalis√©e
            
        Raises:
            ValueError: Si la configuration est invalide
        """
        validated = {}
        
        # Validation des param√®tres num√©riques
        for key, default_value in [
            ('max_pdf_pages', cls.MAX_PDF_PAGES),
            ('max_text_length', cls.MAX_TEXT_LENGTH),
            ('chunk_size', cls.CHUNK_SIZE),
            ('cache_size', cls.CACHE_SIZE)
        ]:
            value = config_dict.get(key, default_value)
            if not isinstance(value, (int, float)) or value <= 0:
                raise ValueError(f"Param√®tre invalide {key}: {value}")
            validated[key] = value
        
        # Validation des bool√©ens
        for key, default_value in [
            ('enable_ocr_corrections', cls.ENABLE_OCR_CORRECTIONS),
            ('enable_validation', cls.ENABLE_VALIDATION)
        ]:
            value = config_dict.get(key, default_value)
            if not isinstance(value, bool):
                raise ValueError(f"Param√®tre invalide {key}: {value}")
            validated[key] = value
        
        return validated
    
class LoggingSetup:
    """
    Configuration et gestion du syst√®me de logging.
    
    Cette classe fournit des m√©thodes pour configurer le syst√®me de logging
    de mani√®re coh√©rente et centralis√©e, avec support pour les logs console
    et fichier.
    """
    
    @staticmethod
    def setup_logging(verbose: bool = False, log_file: Optional[str] = None) -> logging.Logger:
        """
        Configure le syst√®me de logging.
        
        Args:
            verbose: Active le mode debug si True
            log_file: Chemin vers le fichier de log (optionnel)
            
        Returns:
            logging.Logger: Logger configur√©
            
        Raises:
            OSError: Si impossible de cr√©er le r√©pertoire de logs
        """
        try:
            Config.DEFAULT_LOGS_DIR.mkdir(exist_ok=True)
        except OSError as e:
            raise OSError(f"Impossible de cr√©er le r√©pertoire de logs: {e}")
        
        level = logging.DEBUG if verbose else logging.INFO
        
        # Formatter personnalis√© avec couleurs pour la console
        formatter = logging.Formatter(
            '%(asctime)s - %(name)s - %(levelname)s - %(message)s',
            datefmt='%Y-%m-%d %H:%M:%S'
        )
        
        # Logger principal
        logger = logging.getLogger('garmeae_parser')
        logger.setLevel(level)
        
        # Nettoyage des handlers existants
        if logger.handlers:
            logger.handlers.clear()
        
        # Handler console
        console_handler = logging.StreamHandler(sys.stdout)
        console_handler.setLevel(level)
        console_handler.setFormatter(formatter)
        logger.addHandler(console_handler)
        
        # Handler fichier (optionnel)
        if log_file:
            try:
                file_handler = logging.FileHandler(log_file, encoding='utf-8')
                file_handler.setLevel(logging.DEBUG)
                file_handler.setFormatter(formatter)
                logger.addHandler(file_handler)
            except OSError as e:
                logger.warning(f"Impossible de cr√©er le fichier de log {log_file}: {e}")
        
        return logger
    
    @staticmethod
    def get_logger(name: str = None) -> logging.Logger:
        """
        R√©cup√®re un logger configur√©.
        
        Args:
            name: Nom du logger (optionnel)
            
        Returns:
            logging.Logger: Logger configur√©
        """
        return logging.getLogger(name or 'garmeae_parser')

class EnhancedPDFReader:
    """
    Lecteur PDF optimis√© avec gestion d'erreurs avanc√©e.
    
    Cette classe fournit des fonctionnalit√©s avanc√©es pour la lecture et
    l'analyse de documents PDF, avec support pour le traitement par blocs,
    la gestion de la m√©moire et l'optimisation des performances.
    
    Attributes:
        logger: Logger pour les messages de diagnostic
        stats: Statistiques de traitement
        _page_cache: Cache pour les pages d√©j√† lues
        _text_cache: Cache pour les textes extraits
    """
    
    def __init__(self, logger: Optional[logging.Logger] = None):
        """
        Initialise le lecteur PDF.
        
        Args:
            logger: Logger personnalis√© (optionnel)
        """
        self.logger = logger or LoggingSetup.get_logger(__name__)
        
        # Statistiques de traitement
        self.stats = {
            'pages_processed': 0,
            'total_chars': 0,
            'processing_time': 0.0,
            'errors': 0,
            'warnings': 0,
            'cache_hits': 0,
            'cache_misses': 0
        }
        
        # Caches pour optimiser les performances
        self._page_cache = {}
        self._text_cache = {}
        self._max_cache_size = 100
    
    @property
    def can_read_pdf(self) -> bool:
        """
        V√©rifie si la lecture PDF est disponible.
        
        Returns:
            bool: True si PyMuPDF est disponible
        """
        return HAS_PYMUPDF
    
    def _clear_caches(self):
        """Nettoie les caches pour lib√©rer la m√©moire."""
        if len(self._page_cache) > self._max_cache_size:
            self._page_cache.clear()
        if len(self._text_cache) > self._max_cache_size:
            self._text_cache.clear()
    
    def _get_cache_key(self, pdf_path: Path, page_num: int) -> str:
        """G√©n√®re une cl√© de cache unique."""
        return f"{pdf_path.stem}_{page_num}"
    
    def get_pdf_info(self, pdf_path: Union[str, Path]) -> Dict[str, Any]:
        """
        R√©cup√®re les informations d√©taill√©es d'un fichier PDF.
        
        Args:
            pdf_path: Chemin vers le fichier PDF
            
        Returns:
            Dict[str, Any]: Informations d√©taill√©es du PDF
            
        Raises:
            FileNotFoundError: Si le fichier n'existe pas
            PermissionError: Si pas de permission de lecture
        """
        pdf_path = Path(pdf_path)
        
        if not pdf_path.exists():
            raise FileNotFoundError(f"Fichier PDF introuvable: {pdf_path}")
        
        if not pdf_path.is_file():
            raise ValueError(f"Le chemin ne correspond pas √† un fichier: {pdf_path}")
        
        # Informations de base
        basic_info = {
            'file_name': pdf_path.name,
            'file_size_mb': round(pdf_path.stat().st_size / (1024 * 1024), 2),
            'can_process': False,
            'estimated_time_minutes': 0.0,
            'file_path': str(pdf_path.absolute())
        }
        
        if not self.can_read_pdf:
            basic_info['error'] = 'PyMuPDF non disponible'
            return basic_info
        
        try:
            with fitz.open(str(pdf_path)) as doc:
                # Informations du document
                basic_info.update({
                    'pages': len(doc),
                    'can_process': True,
                    'metadata': {
                        'title': doc.metadata.get('title', ''),
                        'author': doc.metadata.get('author', ''),
                        'subject': doc.metadata.get('subject', ''),
                        'creator': doc.metadata.get('creator', ''),
                        'creation_date': doc.metadata.get('creationDate', ''),
                        'modification_date': doc.metadata.get('modDate', '')
                    },
                    'estimated_time_minutes': round(len(doc) * 0.05, 2),
                    'file_format': 'PDF',
                    'version': doc.metadata.get('format', 'PDF')
                })
                
                # Analyse d'un √©chantillon de page
                sample_page = doc[0] if len(doc) > 0 else None
                if sample_page:
                    sample_text = sample_page.get_text()
                    basic_info.update({
                        'has_text': len(sample_text.strip()) > 100,
                        'sample_text_length': len(sample_text),
                        'sample_text_preview': sample_text[:200] + '...' if len(sample_text) > 200 else sample_text,
                        'text_density': round(len(sample_text) / max(sample_page.rect.width * sample_page.rect.height, 1), 4)
                    })
                
        except Exception as e:
            basic_info['error'] = str(e)
            self.logger.error(f"Erreur lecture info PDF: {e}")
            self.stats['errors'] += 1
        
        return basic_info
    
    def read_pdf_file(self, pdf_path: Union[str, Path], 
                     max_pages: Optional[int] = None,
                     page_range: Optional[Tuple[int, int]] = None,
                     progress_callback: Optional[Callable] = None) -> str:
        """
        Lit et extrait le texte d'un fichier PDF avec optimisations.
        
        Args:
            pdf_path: Chemin vers le fichier PDF
            max_pages: Nombre maximum de pages √† traiter
            page_range: Plage de pages sp√©cifique (d√©but, fin)
            progress_callback: Fonction de callback pour le progr√®s
            
        Returns:
            str: Texte extrait du PDF
            
        Raises:
            FileNotFoundError: Si le fichier n'existe pas
            ImportError: Si PyMuPDF n'est pas disponible
            ValueError: Si la plage de pages est invalide
        """
        start_time = time.time()
        pdf_path = Path(pdf_path)
        
        # Validation du fichier
        if not pdf_path.exists():
            raise FileNotFoundError(f"Fichier PDF introuvable: {pdf_path}")
        
        if not pdf_path.is_file():
            raise ValueError(f"Le chemin ne correspond pas √† un fichier: {pdf_path}")
        
        if not self.can_read_pdf:
            raise ImportError("PyMuPDF requis mais non disponible. Installez avec: pip install PyMuPDF")
        
        self.logger.info(f"üìñ Lecture PDF: {pdf_path.name}")
        
        try:
            with fitz.open(str(pdf_path)) as doc:
                total_pages = len(doc)
                self.logger.info(f"üìÑ Document: {total_pages} pages")
                
                # Calcul de la plage de pages
                start_page, end_page = self._calculate_page_range(
                    total_pages, max_pages, page_range)
                
                self.logger.info(f"üéØ Traitement pages {start_page + 1} √† {end_page}")
                
                # Traitement optimis√© par blocs
                text_parts = []
                pages_processed = 0
                total_chars = 0
                
                # Traitement par blocs pour optimiser la m√©moire
                block_size = min(50, end_page - start_page)  # 50 pages par bloc
                
                for block_start in range(start_page, end_page, block_size):
                    block_end = min(block_start + block_size, end_page)
                    
                    for page_num in range(block_start, block_end):
                        try:
                            # V√©rification du cache
                            cache_key = self._get_cache_key(pdf_path, page_num)
                            if cache_key in self._text_cache:
                                page_text = self._text_cache[cache_key]
                                self.stats['cache_hits'] += 1
                            else:
                                page = doc[page_num]
                                page_text = page.get_text()
                                self._text_cache[cache_key] = page_text
                                self.stats['cache_misses'] += 1
                            
                            if page_text.strip():
                                text_parts.append(f"\n--- PAGE {page_num + 1} ---\n")
                                text_parts.append(page_text)
                                total_chars += len(page_text)
                            else:
                                self.logger.warning(f"‚ö†Ô∏è Page {page_num + 1} sans texte")
                                self.stats['warnings'] += 1
                            
                            pages_processed += 1
                            
                            # Mise √† jour du progr√®s
                            if progress_callback:
                                progress = (pages_processed / (end_page - start_page)) * 100
                                progress_callback(progress, page_num + 1, end_page)
                            
                            # Logs de progression
                            if pages_processed % 25 == 0:
                                self.logger.info(f"‚è≥ Progression: {pages_processed}/{end_page - start_page} pages")
                            
                        except Exception as e:
                            self.logger.error(f"‚ùå Erreur page {page_num + 1}: {e}")
                            self.stats['errors'] += 1
                            continue
                    
                    # Nettoyage du cache apr√®s chaque bloc
                    self._clear_caches()
                
                # Assemblage du texte final
                full_text = '\n'.join(text_parts)
                
                # Mise √† jour des statistiques
                processing_time = time.time() - start_time
                self.stats.update({
                    'pages_processed': pages_processed,
                    'total_chars': total_chars,
                    'processing_time': processing_time,
                    'pages_per_second': pages_processed / max(processing_time, 0.001),
                    'chars_per_second': total_chars / max(processing_time, 0.001)
                })
                
                self.logger.info(
                    f"‚úÖ PDF lu avec succ√®s: {pages_processed} pages, "
                    f"{total_chars:,} caract√®res, "
                    f"{processing_time:.2f}s "
                    f"({self.stats['pages_per_second']:.1f} pages/s)"
                )
                
                return full_text
                
        except Exception as e:
            self.stats['errors'] += 1
            self.logger.error(f"‚ùå Erreur lecture PDF: {e}")
            raise
    
    def _calculate_page_range(self, total_pages: int, 
                            max_pages: Optional[int],
                            page_range: Optional[Tuple[int, int]]) -> Tuple[int, int]:
        """
        Calcule la plage de pages √† traiter (0-index√©).
        
        Args:
            total_pages: Nombre total de pages dans le document
            max_pages: Nombre maximum de pages √† traiter
            page_range: Plage sp√©cifique (d√©but, fin) en 1-index√©
            
        Returns:
            Tuple[int, int]: Plage de pages (d√©but, fin) en 0-index√©
            
        Raises:
            ValueError: Si la plage de pages est invalide
        """
        
        if page_range:
            start, end = page_range
            # Validation de la plage
            if start < 1 or end < start:
                raise ValueError(f"Plage invalide: {start}-{end}")
            
            # Convertir en 0-index√© et valider
            start_page = max(0, start - 1)
            end_page = min(total_pages, end)
        else:
            start_page = 0
            end_page = min(total_pages, max_pages) if max_pages else total_pages
        
        # Validation finale
        if start_page >= end_page:
            raise ValueError(f"Plage de pages invalide: {start_page + 1}-{end_page}")
        
        if start_page >= total_pages:
            raise ValueError(f"Page de d√©but {start_page + 1} d√©passe le nombre total de pages {total_pages}")
        
        return start_page, end_page
    
    def get_statistics(self) -> Dict[str, Any]:
        """
        Retourne les statistiques d√©taill√©es de traitement.
        
        Returns:
            Dict[str, Any]: Statistiques compl√®tes avec m√©triques de performance
        """
        stats = self.stats.copy()
        
        # Calcul des m√©triques de performance
        if stats['processing_time'] > 0:
            stats['pages_per_second'] = round(stats['pages_processed'] / stats['processing_time'], 2)
            stats['chars_per_second'] = round(stats['total_chars'] / stats['processing_time'], 2)
            stats['avg_chars_per_page'] = round(stats['total_chars'] / max(stats['pages_processed'], 1), 2)
        
        # M√©triques de cache
        total_cache_operations = stats['cache_hits'] + stats['cache_misses']
        if total_cache_operations > 0:
            stats['cache_hit_rate'] = round(stats['cache_hits'] / total_cache_operations * 100, 2)
        else:
            stats['cache_hit_rate'] = 0.0
        
        # M√©triques de qualit√©
        if stats['pages_processed'] > 0:
            stats['error_rate'] = round(stats['errors'] / stats['pages_processed'] * 100, 2)
            stats['warning_rate'] = round(stats['warnings'] / stats['pages_processed'] * 100, 2)
        else:
            stats['error_rate'] = 0.0
            stats['warning_rate'] = 0.0
        
        # Informations sur la m√©moire
        stats['cache_size'] = len(self._text_cache)
        stats['memory_usage_mb'] = round(
            (len(self._text_cache) * 1024 + len(self._page_cache) * 512) / (1024 * 1024), 2
        )
        
        return stats
    
    def reset_statistics(self):
        """R√©initialise toutes les statistiques."""
        self.stats = {
            'pages_processed': 0,
            'total_chars': 0,
            'processing_time': 0.0,
            'errors': 0,
            'warnings': 0,
            'cache_hits': 0,
            'cache_misses': 0
        }
        self._clear_caches()

class EnhancedGenealogyParser:
    """
    Parseur g√©n√©alogique principal avec int√©gration OCR compl√®te.
    
    Cette classe est le c≈ìur du syst√®me de traitement g√©n√©alogique. Elle coordonne
    l'ensemble des parsers sp√©cialis√©s pour extraire, normaliser et valider les
    informations des documents g√©n√©alogiques fran√ßais de l'Ancien R√©gime.
    
    Fonctionnalit√©s principales:
    - Normalisation et correction OCR du texte
    - Extraction intelligente des noms de personnes
    - Reconnaissance des titres nobiliaires et religieux
    - Gestion des abr√©viations historiques
    - Validation chronologique des donn√©es
    - Cache intelligent pour optimiser les performances
    - Export multi-formats des r√©sultats
    
    Attributes:
        logger: Logger pour les messages de diagnostic
        config: Configuration du parseur
        _text_parser: Parser de texte (lazy loading)
        _name_extractor: Extracteur de noms (lazy loading)
        _person_manager: Gestionnaire de personnes (lazy loading)
        stats: Statistiques de traitement
        _processing_cache: Cache pour optimiser les traitements r√©p√©titifs
    """
    
    def __init__(self, config_path: Optional[str] = None, 
                 logger: Optional[logging.Logger] = None):
        """
        Initialise le parseur g√©n√©alogique.
        
        Args:
            config_path: Chemin vers le fichier de configuration (optionnel)
            logger: Logger personnalis√© (optionnel)
            
        Raises:
            ValueError: Si la configuration est invalide
            OSError: Si le fichier de configuration ne peut √™tre lu
        """
        self.logger = logger or LoggingSetup.get_logger(__name__)
        
        # Chargement et validation de la configuration
        try:
            self.config = self._load_config(config_path)
            self.config = Config.validate_config(self.config)
        except Exception as e:
            self.logger.error(f"Erreur de configuration: {e}")
            raise
        
        # Initialisation des composants (lazy loading)
        self._text_parser = None
        self._name_extractor = None  
        self._person_manager = None
        
        # Cache pour optimiser les traitements
        self._processing_cache = {}
        self._max_cache_size = 1000
        
        # Statistiques de traitement
        self.stats = {
            'documents_processed': 0,
            'total_persons': 0,
            'total_corrections': 0,
            'processing_time': 0.0,
            'errors_handled': 0,
            'cache_hits': 0,
            'cache_misses': 0,
            'ocr_corrections_applied': 0,
            'names_extracted': 0,
            'persons_created': 0
        }
        
        self.logger.info("üîß Parseur g√©n√©alogique initialis√©")
        self.logger.debug(f"Configuration: {self.config}")
    
    def _load_config(self, config_path: Optional[str]) -> Dict[str, Any]:
        """
        Charge la configuration depuis un fichier ou utilise les d√©fauts.
        
        Args:
            config_path: Chemin vers le fichier de configuration (optionnel)
            
        Returns:
            Dict[str, Any]: Configuration charg√©e et valid√©e
            
        Raises:
            OSError: Si le fichier de configuration ne peut √™tre lu
            json.JSONDecodeError: Si le fichier JSON est malform√©
        """
        
        # Configuration par d√©faut
        default_config = {
            'enable_ocr_corrections': Config.ENABLE_OCR_CORRECTIONS,
            'enable_validation': Config.ENABLE_VALIDATION,
            'cache_size': Config.CACHE_SIZE,
            'max_text_length': Config.MAX_TEXT_LENGTH,
            'chunk_size': Config.CHUNK_SIZE,
            'min_name_confidence': Config.MIN_NAME_CONFIDENCE,
            'min_date_confidence': Config.MIN_DATE_CONFIDENCE,
            'min_relationship_confidence': Config.MIN_RELATIONSHIP_CONFIDENCE,
            'progress_update_interval': Config.PROGRESS_UPDATE_INTERVAL,
            'memory_cleanup_threshold': Config.MEMORY_CLEANUP_THRESHOLD
        }
        
        # Chargement depuis fichier si sp√©cifi√©
        if config_path:
            config_file = Path(config_path)
            if config_file.exists():
                try:
                    with open(config_file, 'r', encoding='utf-8') as f:
                        file_config = json.load(f)
                    
                    # Validation des cl√©s de configuration
                    valid_keys = set(default_config.keys())
                    invalid_keys = set(file_config.keys()) - valid_keys
                    if invalid_keys:
                        self.logger.warning(f"Cl√©s de configuration invalides ignor√©es: {invalid_keys}")
                    
                    # Mise √† jour avec les valeurs du fichier
                    default_config.update({k: v for k, v in file_config.items() if k in valid_keys})
                    self.logger.info(f"üìã Configuration charg√©e: {config_path}")
                    
                except json.JSONDecodeError as e:
                    self.logger.error(f"Erreur JSON dans le fichier de configuration: {e}")
                    raise
                except OSError as e:
                    self.logger.error(f"Erreur lecture fichier de configuration: {e}")
                    raise
            else:
                self.logger.warning(f"Fichier de configuration introuvable: {config_path}")
        
        return default_config
    
    def _clear_processing_cache(self):
        """Nettoie le cache de traitement pour lib√©rer la m√©moire."""
        if len(self._processing_cache) > self._max_cache_size:
            self._processing_cache.clear()
            self.logger.debug("Cache de traitement nettoy√©")
    
    def _get_cache_key(self, text: str, operation: str) -> str:
        """G√©n√®re une cl√© de cache unique pour un texte et une op√©ration."""
        content_hash = hashlib.md5(text.encode('utf-8')).hexdigest()
        return f"{operation}_{content_hash[:16]}"
    
    def _update_stats(self, **kwargs):
        """Met √† jour les statistiques de traitement."""
        for key, value in kwargs.items():
            if key in self.stats:
                if isinstance(value, (int, float)):
                    self.stats[key] += value
                else:
                    self.stats[key] = value
    
    @property
    def text_parser(self) -> 'TextParser':
        """
        Parser de texte avec lazy loading.
        
        Returns:
            TextParser: Instance du parser de texte
            
        Raises:
            ImportError: Si les modules parsers ne sont pas disponibles
        """
        if not HAS_PARSERS:
            raise ImportError("Modules parsers non disponibles")
        
        if self._text_parser is None:
            try:
                self._text_parser = TextParser(self.config)
                self.logger.debug("üìù TextParser initialis√©")
            except Exception as e:
                self.logger.error(f"Erreur initialisation TextParser: {e}")
                raise
        
        return self._text_parser
    
    @property
    def name_extractor(self) -> 'NameExtractor':
        """
        Extracteur de noms avec lazy loading.
        
        Returns:
            NameExtractor: Instance de l'extracteur de noms
            
        Raises:
            ImportError: Si les modules parsers ne sont pas disponibles
        """
        if not HAS_PARSERS:
            raise ImportError("Modules parsers non disponibles")
        
        if self._name_extractor is None:
            try:
                self._name_extractor = NameExtractor(self.config)
                self.logger.debug("üë§ NameExtractor initialis√©")
            except Exception as e:
                self.logger.error(f"Erreur initialisation NameExtractor: {e}")
                raise
        
        return self._name_extractor
    
    @property  
    def person_manager(self) -> 'PersonManager':
        """
        Gestionnaire de personnes avec lazy loading.
        
        Returns:
            PersonManager: Instance du gestionnaire de personnes
            
        Raises:
            ImportError: Si les modules parsers ne sont pas disponibles
        """
        if not HAS_PARSERS:
            raise ImportError("Modules parsers non disponibles")
        
        if self._person_manager is None:
            try:
                cache_size = self.config.get('cache_size', Config.CACHE_SIZE)
                self._person_manager = PersonManager(cache_size)
                self.logger.debug("üèõÔ∏è PersonManager initialis√©")
            except Exception as e:
                self.logger.error(f"Erreur initialisation PersonManager: {e}")
                raise
        
        return self._person_manager
    
    def process_document(self, text: str, 
                        source_info: Optional[Dict[str, Any]] = None,
                        progress_callback: Optional[Callable] = None) -> Dict[str, Any]:
        """
        Traite un document g√©n√©alogique complet avec optimisations.
        
        Cette m√©thode coordonne l'ensemble du processus de traitement :
        1. Normalisation et correction OCR du texte
        2. Segmentation du document en sections logiques
        3. Extraction des noms de personnes
        4. Cr√©ation et gestion des entit√©s personnes
        5. Validation et enrichissement des donn√©es
        
        Args:
            text: Texte du document √† traiter
            source_info: Informations sur la source du document
            progress_callback: Fonction de callback pour le progr√®s
            
        Returns:
            Dict[str, Any]: Rapport complet de traitement
            
        Raises:
            ValueError: Si le texte est vide ou invalide
            RuntimeError: Si une erreur critique survient pendant le traitement
        """
        start_time = time.time()
        
        # Validation du texte d'entr√©e
        if not text or not text.strip():
            raise ValueError("Le texte √† traiter ne peut pas √™tre vide")
        
        # Informations de source par d√©faut
        source_info = source_info or {
            'lieu': 'Document g√©n√©alogique',
            'type': 'registre_paroissial',
            'date_traitement': datetime.now().isoformat(),
            'source_id': hashlib.md5(text.encode('utf-8')).hexdigest()[:16]
        }
        
        self.logger.info(f"üöÄ D√©but traitement: {source_info.get('lieu', 'Source inconnue')}")
        self.logger.debug(f"Longueur du texte: {len(text):,} caract√®res")
        
        try:
            # Validation et troncature du texte si n√©cessaire
            if len(text) > self.config['max_text_length']:
                original_length = len(text)
                text = text[:self.config['max_text_length']]
                self.logger.warning(
                    f"Texte tronqu√©: {original_length:,} ‚Üí {len(text):,} caract√®res "
                    f"(limite: {self.config['max_text_length']:,})"
                )
            
            # Initialisation du rapport de traitement
            report = {
                'source_info': source_info,
                'processing_metadata': {
                    'start_time': datetime.now().isoformat(),
                    'text_length': len(text),
                    'enable_ocr': self.config['enable_ocr_corrections'],
                    'enable_validation': self.config['enable_validation'],
                    'parser_version': VERSION
                },
                'results': {},
                'statistics': {},
                'errors': [],
                'warnings': []
            }
            
            # === √âTAPE 1: NORMALISATION DU TEXTE ===
            self._update_progress(progress_callback, 10, "Normalisation du texte...")
            
            try:
                # V√©rification du cache pour la normalisation
                cache_key = self._get_cache_key(text, 'normalization')
                if cache_key in self._processing_cache:
                    norm_result = self._processing_cache[cache_key]
                    self.stats['cache_hits'] += 1
                    self.logger.debug("Normalisation r√©cup√©r√©e du cache")
                else:
                    if HAS_PARSERS:
                        norm_result = self.text_parser.normalize_text(text)
                        self._processing_cache[cache_key] = norm_result
                        self.stats['cache_misses'] += 1
                    else:
                        norm_result = {
                            'normalized': text,
                            'ocr_corrections': [],
                            'abbreviations_expanded': [],
                            'improvement_ratio': 0.0
                        }
                        report['warnings'].append("Modules parsers non disponibles - normalisation basique")
                
                normalized_text = norm_result['normalized']
                
                # Enrichissement du rapport
                report['results']['text_normalization'] = {
                    'ocr_corrections': norm_result.get('ocr_corrections', []),
                    'abbreviations_expanded': norm_result.get('abbreviations_expanded', []),
                    'improvement_ratio': norm_result.get('improvement_ratio', 0.0),
                    'original_length': norm_result.get('original_length', len(text)),
                    'final_length': norm_result.get('final_length', len(normalized_text)),
                    'compression_ratio': norm_result.get('compression_ratio', 1.0)
                }
                
                # Mise √† jour des statistiques
                ocr_corrections_count = len(norm_result.get('ocr_corrections', []))
                self._update_stats(
                    total_corrections=ocr_corrections_count,
                    ocr_corrections_applied=ocr_corrections_count
                )
                
                self.logger.info(
                    f"üìù Normalisation termin√©e: {len(normalized_text):,} caract√®res, "
                    f"{ocr_corrections_count} corrections OCR"
                )
                
            except Exception as e:
                self.logger.error(f"‚ùå Erreur normalisation: {e}")
                normalized_text = text
                report['errors'].append(f"Normalisation: {str(e)}")
                self._update_stats(errors_handled=1)
            
            # === √âTAPE 2: SEGMENTATION ===
            self._update_progress(progress_callback, 25, "Segmentation du document...")
            
            try:
                # Cache pour la segmentation
                cache_key = self._get_cache_key(normalized_text, 'segmentation')
                if cache_key in self._processing_cache:
                    segments = self._processing_cache[cache_key]
                    self.stats['cache_hits'] += 1
                else:
                    if HAS_PARSERS:
                        segments = self.text_parser.extract_segments(normalized_text)
                        self._processing_cache[cache_key] = segments
                        self.stats['cache_misses'] += 1
                    else:
                        segments = [{'type': 'text', 'content': normalized_text}]
                        report['warnings'].append("Modules parsers non disponibles - segmentation basique")
                
                # Analyse des segments
                segments_by_type = self._count_segments_by_type(segments)
                total_segments = len(segments)
                
                report['results']['segmentation'] = {
                    'total_segments': total_segments,
                    'segments_by_type': segments_by_type,
                    'avg_segment_length': sum(len(s.get('text', '')) for s in segments) / max(total_segments, 1),
                    'quality_distribution': self._analyze_segment_quality(segments)
                }
                
                self.logger.info(f"üìã Segmentation termin√©e: {total_segments} segments")
                
            except Exception as e:
                self.logger.error(f"‚ùå Erreur segmentation: {e}")
                segments = [{'type': 'text', 'content': normalized_text}]
                report['errors'].append(f"Segmentation: {str(e)}")
                self._update_stats(errors_handled=1)
            
            # === √âTAPE 3: EXTRACTION DES NOMS ===
            self._update_progress(progress_callback, 50, "Extraction des noms...")
            
            try:
                # Cache pour l'extraction des noms
                cache_key = self._get_cache_key(normalized_text, 'name_extraction')
                if cache_key in self._processing_cache:
                    persons_data = self._processing_cache[cache_key]
                    self.stats['cache_hits'] += 1
                else:
                    if HAS_PARSERS:
                        persons_data = self.name_extractor.extract_names(normalized_text)
                        self._processing_cache[cache_key] = persons_data
                        self.stats['cache_misses'] += 1
                    else:
                        persons_data = []
                        report['warnings'].append("Modules parsers non disponibles - extraction basique")
                
                # Analyse des noms extraits
                total_names = len(persons_data)
                names_with_corrections = sum(1 for p in persons_data if p.get('ocr_corrected', False))
                high_confidence_names = sum(1 for p in persons_data if p.get('confidence', 0) > 0.8)
                
                report['results']['name_extraction'] = {
                    'total_names': total_names,
                    'names_with_corrections': names_with_corrections,
                    'high_confidence_names': high_confidence_names,
                    'avg_confidence': sum(p.get('confidence', 0) for p in persons_data) / max(total_names, 1),
                    'sample_names': [p['full_name'] for p in persons_data[:10]],
                    'name_types': self._analyze_name_types(persons_data)
                }
                
                self._update_stats(names_extracted=total_names)
                
                self.logger.info(
                    f"üë• Extraction termin√©e: {total_names} noms, "
                    f"{names_with_corrections} corrig√©s, "
                    f"{high_confidence_names} haute confiance"
                )
                
            except Exception as e:
                self.logger.error(f"‚ùå Erreur extraction noms: {e}")
                persons_data = []
                report['errors'].append(f"Extraction noms: {str(e)}")
                self._update_stats(errors_handled=1)
            
            # === √âTAPE 4: CR√âATION DES PERSONNES ===
            self._update_progress(progress_callback, 75, "Cr√©ation des personnes...")
            
            created_persons = []
            try:
                if HAS_PARSERS and persons_data:
                    # Traitement par lots pour optimiser les performances
                    batch_size = 50
                    total_batches = (len(persons_data) + batch_size - 1) // batch_size
                    
                    for batch_idx in range(total_batches):
                        start_idx = batch_idx * batch_size
                        end_idx = min(start_idx + batch_size, len(persons_data))
                        batch_data = persons_data[start_idx:end_idx]
                        
                        for person_data in batch_data:
                            try:
                                # Validation des donn√©es de la personne
                                if not person_data.get('full_name'):
                                    continue
                                
                                person = self.person_manager.find_or_create_person(
                                    person_data['full_name'],
                                    {
                                        'source': source_info.get('lieu', 'Source'),
                                        'extraction_data': person_data,
                                        'source_id': source_info.get('source_id', ''),
                                        'extraction_confidence': person_data.get('confidence', 0.0)
                                    }
                                )
                                created_persons.append(person)
                                
                            except Exception as e:
                                self.logger.warning(f"‚ö†Ô∏è Erreur cr√©ation personne '{person_data.get('full_name', 'N/A')}': {e}")
                                continue
                        
                        # Mise √† jour du progr√®s pour les lots
                        if progress_callback:
                            batch_progress = 75 + (batch_idx + 1) / total_batches * 15
                            progress_callback(batch_progress, f"Lot {batch_idx + 1}/{total_batches}")
                    
                    # Statistiques du gestionnaire de personnes
                    person_stats = self.person_manager.get_enhanced_statistics()
                    
                    report['results']['person_creation'] = {
                        'total_persons': len(created_persons),
                        'persons_created': len([p for p in created_persons if p.id_personne]),
                        'persons_found': len([p for p in created_persons if not p.id_personne]),
                        'cache_statistics': person_stats,
                        'avg_confidence': sum(p.confiance for p in created_persons) / max(len(created_persons), 1)
                    }
                    
                    self._update_stats(persons_created=len(created_persons))
                    
                    self.logger.info(
                        f"üèõÔ∏è Cr√©ation termin√©e: {len(created_persons)} personnes "
                        f"({person_stats.get('total_persons', 0)} total en cache)"
                    )
                else:
                    report['results']['person_creation'] = {
                        'status': 'no_data_or_parsers_unavailable',
                        'total_persons': 0
                    }
                    report['warnings'].append("Aucune donn√©e de personne √† traiter")
                
            except Exception as e:
                self.logger.error(f"‚ùå Erreur cr√©ation personnes: {e}")
                report['errors'].append(f"Cr√©ation personnes: {str(e)}")
                self._update_stats(errors_handled=1)
            
            # === FINALISATION ===
            self._update_progress(progress_callback, 90, "Finalisation...")
            
            processing_time = time.time() - start_time
            
            # Mise √† jour des statistiques globales
            self._update_stats(
                documents_processed=1,
                total_persons=len(created_persons),
                processing_time=processing_time
            )
            
            # Nettoyage du cache si n√©cessaire
            if self.stats['documents_processed'] % self.config.get('memory_cleanup_threshold', 1000) == 0:
                self._clear_processing_cache()
                self.logger.debug("Nettoyage p√©riodique du cache effectu√©")
            
            # Finalisation du rapport
            report['processing_metadata'].update({
                'end_time': datetime.now().isoformat(),
                'processing_time_seconds': round(processing_time, 3),
                'errors_count': len(report['errors']),
                'warnings_count': len(report['warnings']),
                'success_rate': self._calculate_success_rate(report)
            })
            
            # G√©n√©ration des statistiques d√©taill√©es
            report['statistics'] = self._generate_processing_statistics()
            
            # Validation finale des r√©sultats
            if self.config.get('enable_validation', True):
                validation_result = self._validate_results(report)
                report['validation'] = validation_result
            
            self._update_progress(progress_callback, 100, "Traitement termin√©!")
            
            self.logger.info(
                f"‚úÖ Traitement termin√© en {processing_time:.2f}s - "
                f"{len(created_persons)} personnes, "
                f"{len(report['errors'])} erreurs, "
                f"{len(report['warnings'])} avertissements"
            )
            
            return report
            
        except Exception as e:
            self._update_stats(errors_handled=1)
            self.logger.error(f"‚ùå Erreur critique traitement: {e}")
            if self.logger.isEnabledFor(logging.DEBUG):
                self.logger.debug(f"Traceback: {traceback.format_exc()}")
            raise
    
    def _count_segments_by_type(self, segments: List[Dict]) -> Dict[str, int]:
        """
        Compte les segments par type.
        
        Args:
            segments: Liste des segments √† analyser
            
        Returns:
            Dict[str, int]: Comptage par type de segment
        """
        counts = defaultdict(int)
        for segment in segments:
            seg_type = segment.get('type', 'unknown')
            counts[seg_type] += 1
        return dict(counts)
    
    def _analyze_segment_quality(self, segments: List[Dict]) -> Dict[str, Any]:
        """
        Analyse la qualit√© des segments.
        
        Args:
            segments: Liste des segments √† analyser
            
        Returns:
            Dict[str, Any]: Analyse de la qualit√©
        """
        if not segments:
            return {'avg_quality': 0.0, 'quality_distribution': {}}
        
        qualities = [s.get('quality_score', 0.0) for s in segments]
        avg_quality = sum(qualities) / len(qualities)
        
        # Distribution de qualit√©
        quality_ranges = {
            'excellent': len([q for q in qualities if q >= 0.8]),
            'good': len([q for q in qualities if 0.6 <= q < 0.8]),
            'fair': len([q for q in qualities if 0.4 <= q < 0.6]),
            'poor': len([q for q in qualities if q < 0.4])
        }
        
        return {
            'avg_quality': round(avg_quality, 3),
            'quality_distribution': quality_ranges,
            'total_segments': len(segments)
        }
    
    def _analyze_name_types(self, persons_data: List[Dict]) -> Dict[str, int]:
        """
        Analyse les types de noms extraits.
        
        Args:
            persons_data: Donn√©es des personnes extraites
            
        Returns:
            Dict[str, int]: Comptage par type de nom
        """
        name_types = defaultdict(int)
        
        for person in persons_data:
            name_type = person.get('name_type', 'unknown')
            name_types[name_type] += 1
            
            # Analyse des titres
            if person.get('has_noble_title'):
                name_types['noble'] += 1
            if person.get('has_religious_title'):
                name_types['religious'] += 1
        
        return dict(name_types)
    
    def _update_progress(self, callback: Optional[Callable], 
                        progress: int, message: str):
        """
        Met √† jour le progr√®s si callback fourni.
        
        Args:
            callback: Fonction de callback pour le progr√®s
            progress: Pourcentage de progression (0-100)
            message: Message descriptif
        """
        if callback and callable(callback):
            try:
                callback(progress, message)
            except Exception as e:
                self.logger.debug(f"Erreur callback progr√®s: {e}")
    
    def _calculate_success_rate(self, report: Dict[str, Any]) -> float:
        """
        Calcule le taux de succ√®s du traitement.
        
        Args:
            report: Rapport de traitement
            
        Returns:
            float: Taux de succ√®s (0.0 √† 1.0)
        """
        total_operations = 0
        successful_operations = 0
        
        # Normalisation
        if 'text_normalization' in report['results']:
            total_operations += 1
            if not report['results']['text_normalization'].get('errors'):
                successful_operations += 1
        
        # Segmentation
        if 'segmentation' in report['results']:
            total_operations += 1
            if report['results']['segmentation'].get('total_segments', 0) > 0:
                successful_operations += 1
        
        # Extraction des noms
        if 'name_extraction' in report['results']:
            total_operations += 1
            if report['results']['name_extraction'].get('total_names', 0) > 0:
                successful_operations += 1
        
        # Cr√©ation des personnes
        if 'person_creation' in report['results']:
            total_operations += 1
            if report['results']['person_creation'].get('total_persons', 0) > 0:
                successful_operations += 1
        
        return round(successful_operations / max(total_operations, 1), 3)
    
    def _validate_results(self, report: Dict[str, Any]) -> Dict[str, Any]:
        """
        Valide les r√©sultats du traitement.
        
        Args:
            report: Rapport de traitement
            
        Returns:
            Dict[str, Any]: R√©sultats de validation
        """
        validation = {
            'is_valid': True,
            'issues': [],
            'recommendations': []
        }
        
        # Validation de la normalisation
        if 'text_normalization' in report['results']:
            norm = report['results']['text_normalization']
            if norm.get('improvement_ratio', 0) < 0.1:
                validation['issues'].append("Faible am√©lioration du texte")
                validation['recommendations'].append("V√©rifier la qualit√© OCR du document source")
        
        # Validation de l'extraction des noms
        if 'name_extraction' in report['results']:
            names = report['results']['name_extraction']
            if names.get('total_names', 0) == 0:
                validation['issues'].append("Aucun nom extrait")
                validation['recommendations'].append("V√©rifier le contenu du document")
            elif names.get('avg_confidence', 0) < 0.6:
                validation['issues'].append("Confiance moyenne faible pour les noms")
                validation['recommendations'].append("Am√©liorer la qualit√© du texte source")
        
        # Validation de la cr√©ation des personnes
        if 'person_creation' in report['results']:
            persons = report['results']['person_creation']
            if persons.get('total_persons', 0) == 0:
                validation['issues'].append("Aucune personne cr√©√©e")
                validation['recommendations'].append("V√©rifier l'extraction des noms")
        
        # Validation globale
        if validation['issues']:
            validation['is_valid'] = False
        
        return validation
    
    def _generate_processing_statistics(self) -> Dict[str, Any]:
        """
        G√©n√®re des statistiques d√©taill√©es du traitement.
        
        Returns:
            Dict[str, Any]: Statistiques compl√®tes
        """
        
        base_stats = {
            'global': self.stats.copy(),
            'performance': {
                'avg_processing_time': round(
                    self.stats['processing_time'] / max(self.stats['documents_processed'], 1), 3
                ),
                'corrections_per_document': round(
                    self.stats['total_corrections'] / max(self.stats['documents_processed'], 1), 2
                ),
                'persons_per_document': round(
                    self.stats['total_persons'] / max(self.stats['documents_processed'], 1), 2
                ),
                'cache_efficiency': round(
                    self.stats['cache_hits'] / max(self.stats['cache_hits'] + self.stats['cache_misses'], 1) * 100, 2
                )
            },
            'quality_metrics': {
                'error_rate': round(
                    self.stats['errors_handled'] / max(self.stats['documents_processed'], 1) * 100, 2
                ),
                'ocr_correction_rate': round(
                    self.stats['ocr_corrections_applied'] / max(self.stats['total_corrections'], 1) * 100, 2
                )
            }
        }
        
        # Statistiques des parsers si disponibles
        if HAS_PARSERS:
            try:
                if self._text_parser:
                    base_stats['text_parser'] = self._text_parser.get_stats()
                if self._person_manager:
                    base_stats['person_manager'] = self._person_manager.get_enhanced_statistics()
            except Exception as e:
                self.logger.debug(f"Erreur r√©cup√©ration stats parsers: {e}")
        
        return base_stats
    
    def export_results(self, report: Dict[str, Any], 
                      output_dir: Path, 
                      formats: List[str] = None) -> Dict[str, str]:
        formats = formats or ['json']
        output_dir.mkdir(parents=True, exist_ok=True)
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        created_files = {}
        
        for format_type in formats:
            try:
                if format_type == 'json':
                    file_path = output_dir / f"rapport_genealogique_{timestamp}.json"
                    with open(file_path, 'w', encoding='utf-8') as f:
                        json.dump(report, f, indent=2, ensure_ascii=False, default=str)
                    created_files['json'] = str(file_path)
                
                elif format_type == 'txt':
                    file_path = output_dir / f"rapport_genealogique_{timestamp}.txt"
                    with open(file_path, 'w', encoding='utf-8') as f:
                        f.write(self._format_report_as_text(report))
                    created_files['txt'] = str(file_path)
                
                elif format_type == 'gedcom':
                    # Placeholder pour export GEDCOM futur
                    self.logger.info("Export GEDCOM non encore impl√©ment√©")
                
            except Exception as e:
                self.logger.error(f"Erreur export {format_type}: {e}")
        
        return created_files
    
    def _format_report_as_text(self, report: Dict[str, Any]) -> str:
        """Formate le rapport en texte lisible"""
        
        lines = [
            "=" * 80,
            "RAPPORT D'ANALYSE G√âN√âALOGIQUE",
            "=" * 80,
            "",
            f"üìÖ Date: {report['processing_metadata'].get('start_time', 'N/A')}",
            f"üìç Source: {report['source_info'].get('lieu', 'N/A')}",
            f"‚è±Ô∏è Temps traitement: {report['processing_metadata'].get('processing_time_seconds', 0):.2f}s",
            "",
            "üîç R√âSULTATS D'EXTRACTION:",
            ""
        ]
        
        # Normalisation du texte
        if 'text_normalization' in report['results']:
            norm = report['results']['text_normalization']
            ocr_count = len(norm.get('ocr_corrections', []))
            abbrev_count = len(norm.get('abbreviations_expanded', []))
            
            lines.extend([
                f"   üìù Corrections OCR appliqu√©es: {ocr_count}",
                f"   üìñ Abr√©viations d√©velopp√©es: {abbrev_count}",
                f"   üìä Ratio d'am√©lioration: {norm.get('improvement_ratio', 0):.2%}",
                ""
            ])
        
        # Extraction des noms
        if 'name_extraction' in report['results']:
            names = report['results']['name_extraction']
            total = names.get('total_names', 0)
            corrected = names.get('names_with_corrections', 0)
            
            lines.extend([
                f"   üë• Noms extraits: {total}",
                f"   ‚úÖ Noms corrig√©s: {corrected}",
                ""
            ])
            
            # √âchantillon de noms
            if names.get('sample_names'):
                lines.append("   üìã √âchantillon de noms:")
                for name in names['sample_names']:
                    lines.append(f"      ‚Ä¢ {name}")
                lines.append("")
        
        # Statistiques
        if 'statistics' in report:
            stats = report['statistics']
            if 'global' in stats:
                glob_stats = stats['global']
                lines.extend([
                    "üìä STATISTIQUES:",
                    "",
                    f"   üìÑ Documents trait√©s: {glob_stats.get('documents_processed', 0)}",
                    f"   üë• Personnes totales: {glob_stats.get('total_persons', 0)}",
                    f"   üîß Corrections totales: {glob_stats.get('total_corrections', 0)}",
                    f"   ‚ö†Ô∏è Erreurs g√©r√©es: {glob_stats.get('errors_handled', 0)}",
                    ""
                ])
        
        # Erreurs
        if report.get('errors'):
            lines.extend([
                "‚ö†Ô∏è ERREURS RENCONTR√âES:",
                ""
            ])
            for error in report['errors']:
                lines.append(f"   ‚Ä¢ {error}")
            lines.append("")
        
        lines.extend([
            "=" * 80,
            "Fin du rapport",
            "=" * 80
        ])
        
        return '\n'.join(lines)

# === UTILITAIRES ===

class ProgressTracker:
    """Gestionnaire de progression pour les op√©rations longues"""
    
    def __init__(self, show_progress: bool = True):
        self.show_progress = show_progress
        self.start_time = time.time()
        self.last_update = 0
    
    def update(self, progress: int, message: str = ""):
        """Met √† jour la progression"""
        if not self.show_progress:
            return
        
        # Limiter les mises √† jour trop fr√©quentes
        current_time = time.time()
        if current_time - self.last_update < 0.5 and progress != 100:
            return
        
        self.last_update = current_time
        elapsed = current_time - self.start_time
        
        # Barre de progression simple
        bar_length = 30
        filled_length = int(bar_length * progress // 100)
        bar = '‚ñà' * filled_length + '‚ñë' * (bar_length - filled_length)
        
        print(f'\r‚è≥ [{bar}] {progress:3d}% {message} ({elapsed:.1f}s)', end='', flush=True)
        
        if progress >= 100:
            print()  # Nouvelle ligne √† la fin

@contextmanager
def safe_file_operation(file_path: Union[str, Path], operation: str = "operation"):
    """Context manager pour op√©rations fichier s√©curis√©es"""
    try:
        yield
    except FileNotFoundError:
        print(f"‚ùå Fichier non trouv√© pour {operation}: {file_path}")
    except PermissionError:
        print(f"‚ùå Permission refus√©e pour {operation}: {file_path}")
    except Exception as e:
        print(f"‚ùå Erreur {operation}: {e}")

# === FONCTION PRINCIPALE ===

def create_argument_parser() -> argparse.ArgumentParser:
    """Cr√©e le parser d'arguments de ligne de commande"""
    
    parser = argparse.ArgumentParser(
        description='Parseur g√©n√©alogique avanc√© pour registres paroissiaux',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Exemples d'utilisation:
  python main.py document.pdf                           # Traitement PDF complet
  python main.py document.txt -o resultats/             # Fichier texte avec sortie personnalis√©e
  python main.py document.pdf --pdf-pages 50           # Limiter √† 50 pages
  python main.py document.pdf --pdf-range 10-30        # Pages 10 √† 30
  python main.py demo                                   # Mode d√©monstration
  python main.py --test                                 # Tests int√©gr√©s

Formats support√©s: .pdf, .txt, .md, .rtf
        """
    )
    
    # Fichier d'entr√©e
    parser.add_argument(
        'input_file', 
        nargs='?',
        help='Fichier √† analyser (PDF, TXT) ou "demo" pour d√©monstration'
    )
    
    # Options de sortie
    output_group = parser.add_argument_group('Options de sortie')
    output_group.add_argument(
        '-o', '--output', 
        type=str, 
        default='output',
        help='R√©pertoire de sortie (d√©faut: ./output)'
    )
    output_group.add_argument(
        '--formats', 
        nargs='+', 
        choices=['json', 'txt', 'gedcom'],
        default=['txt'],
        help='Formats d\'export (d√©faut: txt)'
    )
    
    # Options PDF
    pdf_group = parser.add_argument_group('Options PDF')
    pdf_group.add_argument(
        '--pdf-pages', 
        type=int, 
        help='Nombre maximum de pages PDF √† traiter'
    )
    pdf_group.add_argument(
        '--pdf-range', 
        type=str, 
        help='Plage de pages PDF (ex: 10-50)'
    )
    pdf_group.add_argument(
        '--pdf-info', 
        action='store_true',
        help='Afficher uniquement les informations du PDF'
    )
    
    # Options de traitement
    processing_group = parser.add_argument_group('Options de traitement')
    processing_group.add_argument(
        '--no-ocr', 
        action='store_true',
        help='D√©sactiver les corrections OCR'
    )
    processing_group.add_argument(
        '--config', 
        type=str,
        help='Fichier de configuration personnalis√©'
    )
    processing_group.add_argument(
        '--chunk-size', 
        type=int, 
        default=Config.CHUNK_SIZE,
        help=f'Taille des chunks de traitement (d√©faut: {Config.CHUNK_SIZE})'
    )
    
    # Options de logging
    logging_group = parser.add_argument_group('Options de logging')
    logging_group.add_argument(
        '-v', '--verbose', 
        action='store_true',
        help='Mode verbeux (debug)'
    )
    logging_group.add_argument(
        '--log-file', 
        type=str,
        help='Fichier de log personnalis√©'
    )
    logging_group.add_argument(
        '--no-progress', 
        action='store_true',
        help='D√©sactiver l\'affichage du progr√®s'
    )
    
    # Options sp√©ciales
    special_group = parser.add_argument_group('Options sp√©ciales')
    special_group.add_argument(
        '--test', 
        action='store_true',
        help='Lancer les tests int√©gr√©s'
    )
    special_group.add_argument(
        '--check-deps', 
        action='store_true',
        help='V√©rifier les d√©pendances'
    )
    
    return parser

def check_dependencies() -> Dict[str, bool]:
    """V√©rifie les d√©pendances du syst√®me"""
    
    deps = {
        'PyMuPDF (PDF)': HAS_PYMUPDF,
        'Parsers Garm√©a': HAS_PARSERS
    }
    
    print("üîç V√âRIFICATION DES D√âPENDANCES")
    print("=" * 40)
    
    all_ok = True
    for name, available in deps.items():
        status = "‚úÖ Disponible" if available else "‚ùå Manquant"
        print(f"{name:20} : {status}")
        if not available:
            all_ok = False
    
    print()
    
    if not all_ok:
        print("‚ö†Ô∏è  INSTRUCTIONS D'INSTALLATION:")
        if not HAS_PYMUPDF:
            print("   pip install PyMuPDF")
        if not HAS_PARSERS:
            print("   V√©rifiez que les modules parsers sont dans le PYTHONPATH")
    else:
        print("‚úÖ Toutes les d√©pendances sont disponibles!")
    
    return deps

def run_integrated_tests() -> bool:
    print("üß™ TESTS INT√âGR√âS")
    print("=" * 40)
    
    tests_passed = 0
    total_tests = 0
    
    # Test 1: Configuration
    total_tests += 1
    try:
        config = Config()
        assert hasattr(config, 'MAX_PDF_PAGES')
        print("‚úÖ Test configuration")
        tests_passed += 1
    except Exception as e:
        print(f"‚ùå Test configuration: {e}")
    
    # Test 2: Logging
    total_tests += 1
    try:
        logger = LoggingSetup.setup_logging(verbose=False)
        logger.info("Test log")
        print("‚úÖ Test logging")
        tests_passed += 1
    except Exception as e:
        print(f"‚ùå Test logging: {e}")
    
    # Test 3: PDF Reader (si disponible)
    if HAS_PYMUPDF:
        total_tests += 1
        try:
            pdf_reader = EnhancedPDFReader()
            assert pdf_reader.can_read_pdf
            print("‚úÖ Test PDF reader")
            tests_passed += 1
        except Exception as e:
            print(f"‚ùå Test PDF reader: {e}")
    
    # Test 4: Parser principal
    total_tests += 1
    try:
        parser = EnhancedGenealogyParser()
        assert hasattr(parser, 'process_document')
        print("‚úÖ Test parser principal")
        tests_passed += 1
    except Exception as e:
        print(f"‚ùå Test parser principal: {e}")
    
    print(f"\nüìä R√©sultats: {tests_passed}/{total_tests} tests r√©ussis")
    return tests_passed == total_tests

def run_demo() -> Dict[str, Any]:
    """Lance une d√©monstration avec exemple de texte"""
    
    print("üé≠ MODE D√âMONSTRATION")
    print("=" * 50)
    
    # Texte d'exemple enrichi
    sample_text = """
    1643-1687. ‚Äî Registres de bapt√™mes, mariages et s√©pultures de Notre-Dame d'Esm√©ville.
    
    ¬´ L'an de gr√¢ce 1643, le dimanche 8e jour de mars, moy, Charles Demontigny, prestre, 
    ay, au nom de Dieu, pris possession du b√©n√©fice Notre-Dame d'Esm√©ville, sans aucune opposition. ¬ª
    
    ‚Äî 1646, 13 f√©v., d√©c√®s de Jean Le Boucher, √©cuyer, sieur de Br√©ville. Le 14, inhumation 
    dans l'√©glise.
    
    ‚Äî 1651, 23 janv., inh., dans l'√©glise, de Fran√ßoise Picot, √©pouse de Charles Le Boucher, 
    √©cuyer, sieur du Hausey, avocat du Roi au si√®ge de Saint-Sylvain.
    
    ‚Äî 24 oct. 1651, naissance et bapt. de Charlotte, fille de Jean Le Boucher, √©cuyer, 
    sieur de La Granville, et de Fran√ßoise Varin; marraine: Perrette Dupr√©; 
    parrain: Charles Le Boucher, √©cuyer, sieur du Hozey, conseiller et avocat du Roi.
    
    ‚Äî 1655, 15 mars, mariage de Pierre Martin, fils de Jean Martin, laboureur, 
    avec Marie Durand, fille de Nicolas Durand, marchand.
    """
    
    print("Texte d'exemple:")
    print("-" * 50)
    print(sample_text[:300] + "..." if len(sample_text) > 300 else sample_text)
    print("-" * 50)
    
    try:
        parser = EnhancedGenealogyParser()
        
        source_info = {
            'lieu': 'Notre-Dame d\'Esm√©ville',
            'type': 'demo',
            'periode': '1643-1687'
        }
        
        progress = ProgressTracker(show_progress=True)
        
        print("\nTraitement en cours...")
        report = parser.process_document(
            sample_text, 
            source_info,
            progress_callback=progress.update
        )
        
        print("\nR√âSULTATS:")
        print("=" * 30)
        
        if 'text_normalization' in report['results']:
            norm = report['results']['text_normalization']
            print(f"Corrections OCR: {len(norm.get('ocr_corrections', []))}")
            print(f"Abr√©viations: {len(norm.get('abbreviations_expanded', []))}")
        
        if 'name_extraction' in report['results']:
            names = report['results']['name_extraction']
            print(f"üë• Noms extraits: {names.get('total_names', 0)}")
            
            # Afficher quelques noms
            sample_names = names.get('sample_names', [])[:5]
            if sample_names:
                print("\nNoms trouv√©s:")
                for name in sample_names:
                    print(f"   ‚Ä¢ {name}")
        
        processing_time = report['processing_metadata'].get('processing_time_seconds', 0)
        print(f"\n‚è±Temps de traitement: {processing_time:.2f}s")
        
        return report
        
    except Exception as e:
        print(f"Erreur d√©monstration: {e}")
        if '--verbose' in sys.argv:
            traceback.print_exc()
        return {'error': str(e)}

def main():
    parser = create_argument_parser()
    args = parser.parse_args()
    log_file = args.log_file or Config.DEFAULT_LOGS_DIR / "genealogy_parser.log"
    logger = LoggingSetup.setup_logging(args.verbose, log_file)
    
    if args.check_deps:
        check_dependencies()
        return
    
    if args.test:
        success = run_integrated_tests()
        sys.exit(0 if success else 1)
    
    if not args.input_file:
        print("Aucun fichier sp√©cifi√©. Utilisez --help pour l'aide.")
        sys.exit(1)
    
    if args.input_file.lower() == 'demo':
        run_demo()
        return
    
    input_path = Path(args.input_file)
    if not input_path.exists():
        print(f"Fichier non trouv√©: {input_path}")
        sys.exit(1)
    
    if input_path.suffix.lower() not in Config.get_all_supported_formats():
        print(f"‚ùå Format non support√©: {input_path.suffix}")
        print(f"Formats support√©s: {', '.join(Config.get_all_supported_formats())}")
        sys.exit(1)
    
    # Cr√©ation du r√©pertoire de sortie
    output_dir = Path(args.output)
    output_dir.mkdir(parents=True, exist_ok=True)
    
    logger.info(f"üöÄ D√©marrage traitement: {input_path.name}")
    
    try:
        # === LECTURE DU FICHIER ===
        
        text_content = ""
        source_info = {
            'fichier': input_path.name,
            'type': 'pdf' if input_path.suffix.lower() == '.pdf' else 'text',
            'taille_mb': input_path.stat().st_size / (1024 * 1024)
        }
        
        if input_path.suffix.lower() == '.pdf':
            # Traitement PDF
            pdf_reader = EnhancedPDFReader(logger)
            
            if not pdf_reader.can_read_pdf:
                print("‚ùå PyMuPDF non disponible pour lire les PDF")
                print("   Installation: pip install PyMuPDF")
                sys.exit(1)
            
            # Info PDF si demand√©
            if args.pdf_info:
                info = pdf_reader.get_pdf_info(input_path)
                print("\nüìã INFORMATIONS PDF:")
                print("=" * 30)
                for key, value in info.items():
                    print(f"{key:20}: {value}")
                return
            
            # Options PDF
            pdf_options = {}
            if args.pdf_pages:
                pdf_options['max_pages'] = args.pdf_pages
            
            if args.pdf_range:
                try:
                    start, end = map(int, args.pdf_range.split('-'))
                    pdf_options['page_range'] = (start, end)
                except ValueError:
                    print(f"Format de plage invalide: {args.pdf_range}")
                    sys.exit(1)
            
            progress = ProgressTracker(not args.no_progress)
            print(f"Lecture PDF: {input_path.name}")
            text_content = pdf_reader.read_pdf_file(
                input_path, 
                progress_callback=progress.update,
                **pdf_options)
            
            pdf_stats = pdf_reader.get_statistics()
            logger.info(f"PDF trait√©: {pdf_stats['pages_processed']} pages")
            source_info.update(pdf_stats)
        
        else:
            print(f"Lecture fichier texte: {input_path.name}")
            with safe_file_operation(input_path, "lecture"):
                with open(input_path, 'r', encoding='utf-8') as f:
                    text_content = f.read()
        
        if not text_content.strip():
            print("Aucun contenu textuel extrait!")
            sys.exit(1)
        
        print(f"Contenu extrait: {len(text_content):,} caract√®res")
        config_overrides = {}
        if args.no_ocr:
            config_overrides['enable_ocr_corrections'] = False
        if args.chunk_size:
            config_overrides['chunk_size'] = args.chunk_size
        
        parser_instance = EnhancedGenealogyParser(args.config, logger)
        if config_overrides:
            parser_instance.config.update(config_overrides)
        progress = ProgressTracker(not args.no_progress)
        
        print(f"\nüîÑ Traitement g√©n√©alogique...")
        report = parser_instance.process_document(
            text_content,
            source_info,
            progress_callback=progress.update
        )
        
        print(f"\nExport des r√©sultats...")
        created_files = parser_instance.export_results(
            report, 
            output_dir, 
            args.formats)
        
        print(f"\nR√âSULTATS:")
        print("=" * 50)
        
        if 'text_normalization' in report['results']:
            norm = report['results']['text_normalization']
            ocr_count = len(norm.get('ocr_corrections', []))
            abbrev_count = len(norm.get('abbreviations_expanded', []))
            print(f"Corrections OCR appliqu√©es: {ocr_count}")
            print(f"Abr√©viations d√©velopp√©es: {abbrev_count}")
        
        if 'name_extraction' in report['results']:
            names = report['results']['name_extraction']
            total_names = names.get('total_names', 0)
            corrected_names = names.get('names_with_corrections', 0)
            print(f"Noms extraits: {total_names}")
            print(f"Noms corrig√©s automatiquement: {corrected_names}")
        
        if 'person_creation' in report['results']:
            persons = report['results']['person_creation']
            total_persons = persons.get('total_persons', 0)
            print(f"Personnes cr√©√©es: {total_persons}")
        
        processing_time = report['processing_metadata'].get('processing_time_seconds', 0)
        print(f"‚è±Temps de traitement: {processing_time:.2f}s")
        
        if created_files:
            print(f"\nFichiers cr√©√©s:")
            for format_type, file_path in created_files.items():
                print(f"   {format_type.upper()}: {file_path}")
        
        if report.get('errors'):
            print(f"\nErreurs rencontr√©es: {len(report['errors'])}")
            if args.verbose:
                for error in report['errors']:
                    print(f"   ‚Ä¢ {error}")
        
        if args.verbose:
            print(f"\nSTATISTIQUES D√âTAILL√âES:")
            print("-" * 30)
            stats = report.get('statistics', {})
            print(json.dumps(stats, indent=2, ensure_ascii=False, default=str))
        
        print(f"\nTraitement termin√© avec succ√®s!")
        
    except KeyboardInterrupt:
        print(f"\n‚èπTraitement interrompu par l'utilisateur")
        sys.exit(130)
    
    except Exception as e:
        logger.error(f"Erreur critique: {e}")
        print(f"\nErreur: {e}")
        
        if args.verbose:
            print("\nüîç D√©tails de l'erreur:")
            traceback.print_exc()
        sys.exit(1)
if __name__ == "__main__":
    main()